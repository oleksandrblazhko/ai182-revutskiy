# Перегляньте метафайли веб-сервера на предмет витоку інформації

| ID |
| – |
| WSTG-INFO-03 |

## Резюме
У цьому розділі описано, як перевірити різні файли метаданих на предмет витоку інформації про шлях(и) веб-програми або функціональність. Крім того, список каталогів, яких слід уникати павукам, роботам або сканерам, також може бути створений як залежність для шляхів виконання Map через програму. Також може збиратися інша інформація, щоб ідентифікувати поверхню атаки, деталі технології або використовуватися для соціальної інженерії.

## Цілі тесту
- Визначайте приховані або затуманені шляхи та функціональні можливості за допомогою аналізу файлів метаданих.
- Отримайте та відобразіть іншу інформацію, яка може сприяти кращому розумінню існуючих систем.

## Як тестувати

* Будь-яку з наведених нижче дій за допомогою wget також можна виконати за допомогою curl. Багато інструментів динамічного тестування безпеки додатків (DAST), таких як ZAP і Burp Suite, включають перевірки або розбір цих ресурсів як частину їх функцій павука/сканера. Їх також можна ідентифікувати за допомогою різних Google Dorks або розширених функцій пошуку, таких як inurl: *

### Роботи
Веб-павуки, роботи чи сканери отримують веб-сторінку, а потім рекурсивно переходять за гіперпосиланнями, щоб отримати подальший веб-вміст. Їх прийнятна поведінка визначається протоколом виключення роботів файлу robots.txt у кореневому каталозі веб-сайту.

Як приклад, нижче наведено початок файлу robots.txt від Google, взятого 5 травня 2020 року:

```
User-agent: *
Disallow: /search
Allow: /search/about
Allow: /search/static
Allow: /search/howsearchworks
Disallow: /sdch
...
```

Директива User-Agent стосується конкретного веб-павука/робота/сканера. Наприклад, User-Agent: Googlebot відноситься до павука від Google, тоді як User-Agent: bingbot відноситься до сканера від Microsoft. User-Agent: * у наведеному вище прикладі стосується всіх веб-павуків/роботів/сканерів.

Директива Disallow визначає, які ресурси заборонені павуками/роботами/сканерами. У наведеному вище прикладі заборонено:

```
...
Disallow: /search
...
Disallow: /sdch
...
```

Веб-павуки/роботи/сканери можуть навмисно ігнорувати директиви Disallow, указані у файлі robots.txt, наприклад директиви з соціальних мереж, щоб переконатися, що спільні посилання залишаються дійсними. Таким чином, robots.txt не слід розглядати як механізм для застосування обмежень щодо того, як треті сторони отримують доступ до веб-контенту, зберігають його чи перепублікують.

Файл robots.txt отримується з кореневого каталогу веб-сервера. Наприклад, щоб отримати robots.txt із www.google.com за допомогою wget або curl:

```
$ curl -O -Ss http://www.google.com/robots.txt && head -n5 robots.txt
User-agent: *
Disallow: /search
Allow: /search/about
Allow: /search/static
Allow: /search/howsearchworks
...
```

#### Проаналізуйте robots.txt за допомогою Google Webmaster Tools
Власники веб-сайтів можуть використовувати функцію Google «Analyze robots.txt» для аналізу веб-сайту в рамках інструментів Google Webmaster Tools. Цей інструмент може допомогти з тестуванням, і процедура така:
- Увійдіть у Google Webmaster Tools за допомогою облікового запису Google.
- На інформаційній панелі введіть URL-адресу сайту для аналізу.
- Виберіть один із доступних методів і дотримуйтесь інструкцій на екрані.

### МЕТА-теги
Теги <META> розташовані в розділі HEAD кожного документа HTML і повинні бути узгодженими на веб-сайті у випадку, якщо початкова точка робота/павука/сканера не починається з посилання на документ, окрім webroot, тобто глибокого посилання. Директиву щодо роботів також можна вказати за допомогою спеціального МЕТА-тегу.

#### Мета-тег роботів
Якщо немає запису <META NAME="ROBOTS" ... >, тоді «Протокол виключення роботів» за замовчуванням має значення INDEX,FOLLOW відповідно. Таким чином, інші два дійсні записи, визначені «Протоколом виключення роботів», мають префікс NO..., тобто NOINDEX і NOFOLLOW.

На основі директив Disallow, указаних у файлі robots.txt у webroot, здійснюється пошук за регулярним виразом <META NAME="ROBOTS" на кожній веб-сторінці, а результат порівнюється з файлом robots.txt у webroot.

#### Теги різної метаінформації
Організації часто вбудовують інформаційні МЕТА-теги у веб-контент для підтримки різних технологій, таких як програми зчитування екрана, попередній перегляд соціальних мереж, індексація пошуковими системами тощо. Така метаінформація може бути корисною для тестувальників у визначенні використовуваних технологій і додаткових шляхів/функціональних можливостей для вивчення і тест. Наступну метаінформацію було отримано з www.whitehouse.gov через View Page Source 5 травня 2020 р.:

```
...
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="The White House" />
<meta property="og:description" content="We, the citizens of America, are now joined in a great national effort to rebuild our country and to restore its promise for all. – President Donald Trump." />
<meta property="og:url" content="https://www.whitehouse.gov/" />
<meta property="og:site_name" content="The White House" />
<meta property="fb:app_id" content="1790466490985150" />
<meta property="og:image" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta property="og:image:secure_url" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:description" content="We, the citizens of America, are now joined in a great national effort to rebuild our country and to restore its promise for all. – President Donald Trump." />
<meta name="twitter:title" content="The White House" />
<meta name="twitter:site" content="@whitehouse" />
<meta name="twitter:image" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta name="twitter:creator" content="@whitehouse" />
...
<meta name="apple-mobile-web-app-title" content="The White House">
<meta name="application-name" content="The White House">
<meta name="msapplication-TileColor" content="#0c2644">
<meta name="theme-color" content="#f5f5f5">
...
```

### Карти сайту
Карта сайту – це файл, у якому розробник або організація може надати інформацію про сторінки, відео та інші файли, які пропонує сайт або програма, а також зв’язок між ними. Пошукові системи можуть використовувати цей файл для більш розумного дослідження вашого сайту. Тестувальники можуть використовувати файли sitemap.xml, щоб дізнатися більше про сайт або програму та вивчити їх більш повно.

Наступний уривок є з основної карти сайту Google, отриманої 5 травня 2020 року.

```
$ wget --no-verbose https://www.google.com/sitemap.xml && head -n8 sitemap.xml
2020-05-05 12:23:30 URL:https://www.google.com/sitemap.xml [2049] -> "sitemap.xml" [1]

<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.google.com/schemas/sitemap/0.84">
  <sitemap>
    <loc>https://www.google.com/gmail/sitemap.xml</loc>
  </sitemap>
  <sitemap>
    <loc>https://www.google.com/forms/sitemaps.xml</loc>
  </sitemap>
...
```

### Security TXT
security.txt — запропонований стандарт, який дозволяє веб-сайтам визначати політику безпеки та контактні дані. Існує кілька причин, чому це може зацікавити сценарії тестування, включаючи, але не обмежуючись:

- Визначення подальших шляхів або ресурсів для включення в відкриття/аналіз.
- Збір розвідувальних даних з відкритим кодом.
- Пошук інформації про баунті тощо.
- Соціальна інженерія.

Файл може бути присутнім у корені веб-сервера або в каталозі .well-known/. Наприклад:
- https://example.com/security.txt
- https://example.com/.well-known/security.txt
Ось реальний приклад, отриманий з LinkedIn 2020 5 травня:

```
$ wget --no-verbose https://www.linkedin.com/.well-known/security.txt && cat security.txt
2020-05-07 12:56:51 URL:https://www.linkedin.com/.well-known/security.txt [333/333] -> "security.txt" [1]
# Conforms to IETF `draft-foudil-securitytxt-07`
Contact: mailto:security@linkedin.com
Contact: https://www.linkedin.com/help/linkedin/answer/62924
Encryption: https://www.linkedin.com/help/linkedin/answer/79676
Canonical: https://www.linkedin.com/.well-known/security.txt
Policy: https://www.linkedin.com/help/linkedin/answer/62924
```

### Humans TXT
humans.txt — це ініціатива для ознайомлення з людьми, які стоять за веб-сайтом. Він має форму текстового файлу, який містить інформацію про різних людей, які зробили внесок у створення веб-сайту. Перегляньте humanstxt для отримання додаткової інформації. Цей файл часто (хоча і не завжди) містить інформацію про кар’єру чи роботу на сайтах/шляхах.

Наступний приклад було отримано з Google 5 травня 2020 р.:
```
$ wget --no-verbose  https://www.google.com/humans.txt && cat humans.txt
2020-05-07 12:57:52 URL:https://www.google.com/humans.txt [286/286] -> "humans.txt" [1]
Google is built by a large team of engineers, designers, researchers, robots, and others in many different sites across the globe. It is updated continuously, and built with more tools and technologies than we can shake a stick at. If you'd like to help us out, see careers.google.com.
```

### Інші добре відомі джерела інформації
Існують інші RFC та чернетки Інтернету, які пропонують стандартизоване використання файлів у каталозі .well-known/. Списки яких можна знайти [тут](https://en.wikipedia.org/wiki/Well-known_URI#List_of_well-known_URIs) або [тут](https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml).

Для тестувальника було б досить просто переглянути RFC/чернетки, створити список, який буде надано сканеру чи фаззеру, щоб перевірити існування чи вміст таких файлів.

## Інструменти
- Браузер (перегляд вихідного коду або функція Dev Tools)
- curl
- wget
- Burp Suite
- ZAP